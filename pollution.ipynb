{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNXy8jC7knmFS+GyxSiriYY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zJKLvLP_B1lt"},"outputs":[],"source":["import os\n","import pandas as pd\n","from datetime import datetime\n","from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n","\n","def parse(x):\n","    return datetime.strptime(x, '%Y %m %d %H')\n","\n","# Check if running in Google Colab\n","try:\n","    from google.colab import drive\n","    IN_COLAB = True\n","except:\n","    IN_COLAB = False\n","\n","# Define the file path\n","file_name = 'polution for students.csv'\n","dir_path = os.getcwd()\n","\n","if IN_COLAB:\n","    # Mount Google Drive\n","    drive.mount('/content/gdrive')\n","    dir_path = '/content/gdrive/My Drive'\n","    data_dir = os.path.join(dir_path, 'DeepLearning/student pollution')\n","    file_path = os.path.join(data_dir, file_name)\n","else:\n","    # Local environment\n","    data_dir = os.path.join(dir_path, 'student pollution')\n","    file_path = os.path.join(data_dir, file_name)\n","\n","# Check if the file exists\n","if not os.path.exists(file_path):\n","    print(\"File not found:\", file_path)\n","else:\n","    # Load the dataset\n","    dataset = pd.read_csv(file_path)\n","\n","    print(dataset.head())"]},{"cell_type":"code","source":["from pandas import to_datetime\n","\n","# Combining year, month, day, and hour into a datetime index\n","dataset['date'] = to_datetime(dataset[['year', 'month', 'day', 'hour']])\n","dataset.set_index('date', inplace=True)\n","\n","# Dropping unnecessary columns\n","dataset.drop(['No', 'year', 'month', 'day', 'hour', 'station'], axis=1, inplace=True)\n","\n","# Handling missing values\n","dataset.fillna(0, inplace=True)\n","\n","# Encoding categorical data\n","dataset['wd'] = dataset['wd'].astype('category').cat.codes\n","\n","# Normalize the dataset\n","from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","scaled = scaler.fit_transform(dataset)\n","\n","# Confirming changes\n","print(dataset.head())\n"],"metadata":{"id":"Bx-clqJpEgJP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pandas import DataFrame, concat\n","\n","def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n","    n_vars = 1 if type(data) is list else data.shape[1]\n","    df = DataFrame(data)\n","    cols, names = list(), list()\n","\n","    # Input sequence (t-n, ... t-1)\n","    for i in range(n_in, 0, -1):\n","        cols.append(df.shift(i))\n","        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n","\n","    # Forecast sequence (t, t+1, ... t+n)\n","    for i in range(0, n_out):\n","        cols.append(df.shift(-i))\n","        if i == 0:\n","            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n","        else:\n","            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n","\n","    # Put it all together\n","    agg = concat(cols, axis=1)\n","    agg.columns = names\n","\n","    # Drop rows with NaN values\n","    if dropnan:\n","        agg.dropna(inplace=True)\n","\n","    return agg\n","\n"],"metadata":{"id":"HfF4qFzNEobJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Using series_to_supervised for 2-day ahead prediction\n","reframed_2_days = series_to_supervised(scaled, n_in=1, n_out=2)\n","\n","# Drop columns not needed for 2-day prediction\n","pm25_col_index = 0\n","n_features = scaled.shape[1]\n","columns_to_drop = [f'var{j}(t+1)' for j in range(1, n_features + 1) if j != pm25_col_index]\n","\n","reframed_2_days.drop(columns_to_drop, axis=1, inplace=True)\n","\n","\n","# Drop columns we don't need for 3-day prediction\n","\n","reframed_3_days = series_to_supervised(scaled, n_in=2, n_out=3)\n","\n","columns_to_drop_3_days = [f'var{j}(t+{i})' for i in [1, 2] for j in range(1, n_features + 1) if j != pm25_col_index]\n","\n","reframed_3_days.drop(columns_to_drop_3_days, axis=1, inplace=True)\n","\n","\n","print(reframed_2_days.head())\n","print(reframed_3_days.head())\n"],"metadata":{"id":"jcbyFy6BFgTU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Splitting the data\n","values_2_days = reframed_2_days.values\n","n_train_hours = int(round(len(values_2_days) * 0.6))\n","n_valid_hours = int(round(len(values_2_days) * 0.8))\n","\n","train_2_days = values_2_days[:n_train_hours, :]\n","valid_2_days = values_2_days[n_train_hours:n_valid_hours, :]\n","test_2_days = values_2_days[n_valid_hours:, :]\n","\n","# Split into input and output\n","n_obs = n_features * 1  # Number of features times number of input time steps\n","\n","train_X_2_days, train_y_2_days = train_2_days[:, :n_obs], train_2_days[:, -n_features]\n","valid_X_2_days, valid_y_2_days = valid_2_days[:, :n_obs], valid_2_days[:, -n_features]\n","test_X_2_days, test_y_2_days = test_2_days[:, :n_obs], test_2_days[:, -n_features]\n","\n","# Reshape input to be 3D [samples, timesteps, features]\n","train_X_2_days = train_X_2_days.reshape((train_X_2_days.shape[0], 1, train_X_2_days.shape[1]))\n","valid_X_2_days = valid_X_2_days.reshape((valid_X_2_days.shape[0], 1, valid_X_2_days.shape[1]))\n","test_X_2_days = test_X_2_days.reshape((test_X_2_days.shape[0], 1, test_X_2_days.shape[1]))\n"],"metadata":{"id":"aySrtU8QHM_E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import LSTM, Dense\n","\n","\n","# Define the LSTM model\n","model_2_days = Sequential()\n","model_2_days.add(LSTM(50, input_shape=(train_X_2_days.shape[1], train_X_2_days.shape[2])))\n","model_2_days.add(Dense(1))\n","model_2_days.compile(loss='mae', optimizer='adam', metrics=['mse'])\n","\n","# Fit the model\n","history_2_days = model_2_days.fit(train_X_2_days, train_y_2_days, epochs=50, batch_size=90,\n","                                  validation_data=(valid_X_2_days, valid_y_2_days), verbose=2, shuffle=False)\n","\n","# Evaluate the model\n","test_loss_2_days = model_2_days.evaluate(test_X_2_days, test_y_2_days, verbose=0)\n","print('Test Loss for 2-day prediction:', test_loss_2_days)\n","\n"],"metadata":{"id":"WuDejdI0HPFE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Splitting the data for 3-day prediction\n","values_3_days = reframed_3_days.values\n","n_train_hours = int(round(len(values_3_days) * 0.6))\n","n_valid_hours = int(round(len(values_3_days) * 0.8))\n","\n","train_3_days = values_3_days[:n_train_hours, :]\n","valid_3_days = values_3_days[n_train_hours:n_valid_hours, :]\n","test_3_days = values_3_days[n_valid_hours:, :]\n","\n","# Split into input and output\n","n_obs = n_features * 2  # Number of features times number of input time steps for 3-day prediction\n","\n","train_X_3_days, train_y_3_days = train_3_days[:, :n_obs], train_3_days[:, -n_features]\n","valid_X_3_days, valid_y_3_days = valid_3_days[:, :n_obs], valid_3_days[:, -n_features]\n","test_X_3_days, test_y_3_days = test_3_days[:, :n_obs], test_3_days[:, -n_features]\n","\n","# Reshape input to be 3D [samples, timesteps, features]\n","train_X_3_days = train_X_3_days.reshape((train_X_3_days.shape[0], 2, int(train_X_3_days.shape[1]/2)))\n","valid_X_3_days = valid_X_3_days.reshape((valid_X_3_days.shape[0], 2, int(valid_X_3_days.shape[1]/2)))\n","test_X_3_days = test_X_3_days.reshape((test_X_3_days.shape[0], 2, int(test_X_3_days.shape[1]/2)))\n"],"metadata":{"id":"BHFFVt83IN25"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the LSTM model for 3-day prediction\n","model_3_days = Sequential()\n","model_3_days.add(LSTM(50, input_shape=(train_X_3_days.shape[1], train_X_3_days.shape[2])))\n","model_3_days.add(Dense(1))\n","model_3_days.compile(loss='mae', optimizer='adam',metrics=['mse'])\n","\n","# Fit the model\n","history_3_days = model_3_days.fit(train_X_3_days, train_y_3_days, epochs=50, batch_size=90,\n","                                  validation_data=(valid_X_3_days, valid_y_3_days), verbose=2, shuffle=False)\n","\n","# Evaluate the model\n","test_loss_3_days = model_3_days.evaluate(test_X_3_days, test_y_3_days, verbose=0)\n","print('Test Loss for 3-day prediction:', test_loss_3_days)\n"],"metadata":{"id":"63Tw7MDgITQ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","#2-day prediction\n","\n","# MAE plot\n","plt.figure(figsize=(12, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(history_2_days.history['loss'], label='Train MAE')\n","plt.plot(history_2_days.history['val_loss'], label='Validation MAE')\n","plt.title('2-Day Prediction Model MAE')\n","plt.xlabel('Epochs')\n","plt.ylabel('MAE')\n","plt.legend()\n","\n","# MSE plot\n","plt.subplot(1, 2, 2)\n","plt.plot(history_2_days.history['mse'], label='Train MSE')\n","plt.plot(history_2_days.history['val_mse'], label='Validation MSE')\n","plt.title('2-Day Prediction Model MSE')\n","plt.xlabel('Epochs')\n","plt.ylabel('MSE')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"hbZmtg-GI281"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#for 3-day prediction\n","\n","# MAE plot\n","\n","plt.figure(figsize=(12, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(history_3_days.history['loss'], label='Train MAE')\n","plt.plot(history_3_days.history['val_loss'], label='Validation MAE')\n","plt.title('3-Day Prediction Model MAE')\n","plt.xlabel('Epochs')\n","plt.ylabel('MAE')\n","plt.legend()\n","\n","# MSE plot\n","plt.subplot(1, 2, 2)\n","plt.plot(history_3_days.history['mse'], label='Train MSE')\n","plt.plot(history_3_days.history['val_mse'], label='Validation MSE')\n","plt.title('3-Day Prediction Model MSE')\n","plt.xlabel('Epochs')\n","plt.ylabel('MSE')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"x14MbnMsKQFM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.metrics import mean_squared_error\n","import numpy as np\n","\n","# Predictions for 2-day and 3-day models\n","test_pred_2_days = model_2_days.predict(test_X_2_days)\n","test_pred_3_days = model_3_days.predict(test_X_3_days)\n","\n","def calculate_rmse(actual, predicted):\n","    return np.sqrt(mean_squared_error(actual, predicted))\n","\n","# RMSE for the scaled test set\n","rmse_scaled_2_days = calculate_rmse(test_y_2_days, test_pred_2_days)\n","rmse_scaled_3_days = calculate_rmse(test_y_3_days, test_pred_3_days)\n","\n","print('Test RMSE (Scaled) - 2 days:', rmse_scaled_2_days)\n","print('Test RMSE (Scaled) - 3 days:', rmse_scaled_3_days)\n","\n","# inverse transform\n","test_y_2_days_2d = test_y_2_days.reshape(len(test_y_2_days), 1)\n","test_y_3_days_2d = test_y_3_days.reshape(len(test_y_3_days), 1)\n","\n","# Invert scaling for actual values\n","test_y_2_days_inv = scaler.inverse_transform(np.concatenate((test_y_2_days_2d, np.zeros((len(test_y_2_days_2d), n_features-1))), axis=1))[:, 0]\n","test_y_3_days_inv = scaler.inverse_transform(np.concatenate((test_y_3_days_2d, np.zeros((len(test_y_3_days_2d), n_features-1))), axis=1))[:, 0]\n","\n","# Invert scaling for predicted values\n","test_pred_2_days_inv = scaler.inverse_transform(np.concatenate((test_pred_2_days, np.zeros((len(test_pred_2_days), n_features-1))), axis=1))[:, 0]\n","test_pred_3_days_inv = scaler.inverse_transform(np.concatenate((test_pred_3_days, np.zeros((len(test_pred_3_days), n_features-1))), axis=1))[:, 0]\n","\n","# Calculate RMSE for absolute values\n","rmse_absolute_2_days = calculate_rmse(test_y_2_days_inv, test_pred_2_days_inv)\n","rmse_absolute_3_days = calculate_rmse(test_y_3_days_inv, test_pred_3_days_inv)\n","\n","print('Test RMSE (Absolute) - 2 days:', rmse_absolute_2_days)\n","print('Test RMSE (Absolute) - 3 days:', rmse_absolute_3_days)\n"],"metadata":{"id":"Ttr_l5muLKJ8"},"execution_count":null,"outputs":[]}]}